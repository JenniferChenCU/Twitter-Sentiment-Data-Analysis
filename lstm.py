# -*- coding: utf-8 -*-
"""Applied ML LSTM Implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nkwtSGCfLRhOxC6Xf3kYow-A20p4jE0-

## Data Preprocessing (Replacing URL, Emoji, @username)

### Source idea: 
https://www.kaggle.com/stoicstatic/twitter-sentiment-analysis-for-beginners/notebook
"""

emojiPattern = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad',  ':-(': 'sad', ':-<': 'sad', 
          ':P': 'raspberry', ':O': 'surprised', ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\': 'annoyed', 
          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy', '@@': 'eyeroll', ';)': 'wink', 
          ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused', '<(-_-)>': 'robot', 'd[-_-]b': 'dj', 
          ":'-)": 'sadsmile', ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}

urlPattern = r"((http://)[^ ]*|(https://)[^ ]*|( www\.)[^ ]*)"
userPattern = '@[^\s]+'

new_text = []
for i in range(len(sentimental_df.text)):
    each_tweet = sentimental_df.text.iloc[i].lower()

    modified_tweet = re.sub(urlPattern,' URL', each_tweet)
    for emoji in emojiPattern.keys():
        modified_tweet = modified_tweet.replace(emoji, "EMOJI_" + emojiPattern[emoji])        
    modified_tweet = re.sub(userPattern,' USERNAME', modified_tweet)  
    new_text.append(modified_tweet)

sentimental_df['text'] = pd.Series(new_text)

sentimental_df.text

"""# LSTM

"""

from keras.models import Sequential,Model
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout
from keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from nltk.corpus import stopwords
from keras.layers import *
import keras.backend as kb
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from keras.preprocessing.sequence import pad_sequences
import pandas as pd
import numpy as np

sentimental_df = pd.read_csv('Sentimental_df_clean.csv')
# X_train_embmean = pd.read_csv('X_train_embmean.csv',header=None).values
# X_val_embmean = pd.read_csv('X_val_embmean.csv',header=None).values
# X_test_embmean = pd.read_csv('X_test_embmean.csv',header=None).values
# text = sentimental_df['text']
# y = sentimental_df['target']

sentimental_df = sentimental_df[['text','target']]
sentimental_df

import nltk
nltk.download('stopwords')
def clean(df,stop_words):
  sentimental_df['text'] = sentimental_df['text'].apply(lambda x: ' '.join(x.lower() for x in x.split()))
  sentimental_df['text'] = sentimental_df['text'].apply(lambda x: ' '.join(x for x in x.split() if x not in stop_words))
  return sentimental_df
#  sentimental_df['text'] = sentimental_df['text'].apply(lambda x: [' '.join(x for x in x.split() if x not in stop_words]))
stop_words = stopwords.words('english')
sent_df = clean(sentimental_df,stop_words)

import nltk
nltk.download('stopwords')

text = sent_df['text']
y = sent_df['target']
Y = pd.get_dummies(y).values
tokenizer = Tokenizer(num_words=500, split=' ') 
tokenizer.fit_on_texts(text.values)
X = tokenizer.texts_to_sequences(text.values)
X = pad_sequences(X)

# from sklearn.model_selection import train_test_split
# X_dev, X_test, y_dev, y_test = train_test_split(X,y, test_size=0.2, random_state=42)
# X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.25, random_state=42)



X_train_embmean.shape

X_train.values

from sklearn.model_selection import train_test_split

model = Sequential()
model.add(Embedding(500, 120, input_length = X.shape[1]))
model.add(SpatialDropout1D(0.4))
model.add(LSTM(176, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(2,activation='softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])
print(model.summary())

X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=42)
#X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.25, random_state=42)

batch_size=256
model.fit(X_train, y_train, epochs = 5, validation_split= 0.25,batch_size=batch_size,validation_steps=100, verbose = 'auto')

predictions = model.predict(X_test)

y_pred = np.argmax(predictions,axis=1)
y_test_shaped = np.argmax(y_test,axis=1)

# 
from sklearn.metrics import roc_auc_score
print('Accuracy Score : ' + str(accuracy_score(y_test_shaped,y_pred)))
print('Precision Score : ' + str(precision_score(y_test_shaped,y_pred)))
print('Recall Score : ' + str(recall_score(y_test_shaped,y_pred)))
print('F1 Score : ' + str(f1_score(y_test_shaped,y_pred)))
print('AUC ROC Score: '+ str(roc_auc_score(y_test_shaped,predictions[:,1])))

confusion_matrix(y_test_shaped,y_pred)

text = sent_df['text']
y = sent_df['target']
Y = pd.get_dummies(y).values
tokenizer = Tokenizer(num_words=500, split=' ') 
tokenizer.fit_on_texts(text.values)
X = tokenizer.texts_to_matrix(text.values,mode='tfidf')
X = pad_sequences(X)

model_ = Sequential()
model_.add(Embedding(500, 120, input_length = X.shape[1]))
model_.add(SpatialDropout1D(0.4))
model_.add(LSTM(176, dropout=0.2, recurrent_dropout=0.2))
model_.add(Dense(2,activation='softmax'))
model_.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])
print(model_.summary())

X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=42)
batch_size=256
model.fit(X_train, y_train, epochs = 5, validation_split= 0.25,batch_size=batch_size,validation_steps=100, verbose = 'auto')

predictions = model.predict(X_test)
y_pred = np.argmax(predictions,axis=1)
y_test_shaped = np.argmax(y_test,axis=1)

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

cd 'drive/Shareddrives/Applied Machine Learning'

"""# ROUGH CODE """

def generator_create(batch_size=128):
    input_arr = np.zeros((batch_size,100))
    output_arr = np.zeros((batch_size,2))
    count = 1
    while True:
        for i in range(0,X_train_embmean.shape[0]):
            input_arr[i] = X_train_embmean[i]
            output_arr[i][y_train[i]] = 1
            
            if count == 128:
                temp_sequence = (input_arr,output_arr)
                count = 0
                input_arr = np.zeros((batch_size,100))
                output_arr = np.zeros((batch_size,2))
                yield temp_sequence
            
            count+=1    
                


def develop_model():
    model_1 = Sequential()
    model_1.add(LSTM(128,return_sequences = True,input_shape=(128,100)))
    model_1.add(Dropout(0.2))
    model_1.add(LSTM(24))
    model_1.add(Dropout(0.2))
    model_1.add(Dense(2,activation='softmax'))
    model_1.compile(optimizer = 'adam', loss = 'binary_crossentropy')
    model_1.summary()
    return model_1

def develop_model_without_gen(X_train):
    model_1 = Sequential()
    # model_1.add(LSTM(128))
    model_1.add(Dense(128,input_dim=(X_train.shape[1])))
    # model_1.add(LSTM(128,return_sequences = True))
    # model_1.add(LSTM(128,return_sequences = True,input_shape=(X_train.shape[1],X_train_shape[2])))
    model_1.add(Dropout(0.2))
    model_1.add(Dense(2,activation='softmax'))
    model_1.compile(optimizer = 'adam', loss = 'binary_crossentropy')
    model_1.summary()
    return model_1

# model_1 = Sequential()
# model_1.add(LSTM(128,return_sequences = True,input_shape=(128,100)))
# model_1.add(Dropout(0.2)) 
# model_1.add(Dense(2,activation='softmax'))

# model_1.compile(optimizer = 'adam', loss = 'binary_crossentropy')

# model_1.summary()
# print(model_1.output_shape)

# batch_size = 128
# MAX_LEN = 100
# generator = generator_create(batch_size)
# steps = len(y_train) * MAX_LEN // batch_size
# model = develop_model()
# model.fit_generator(generator,steps_per_epoch = steps,verbose=True,epochs=5)

np.array(y_train).reshape(-1,1).shape

Y_train = np.array(y_train)
y_train_one_hot = np.zeros((Y_train.shape[0], Y_train.max()+1))
y_train_one_hot[np.arange(Y_train.size),Y_train] = 1
X_train_reshaped = np.reshape(X_train_embmean, (X_train_embmean.shape[0],1,X_train_embmean.shape[1]))

X_train_embmean.shape

X_train.shape

X_train_reshaped.shape

X_train_reshaped.squeeze().shape

model = develop_model_without_gen(X_train_reshaped.squeeze())
model.fit(X_train_reshaped.squeeze(),y_train_one_hot,epochs=5,batch_size=32)

X_train_embmean.shape

